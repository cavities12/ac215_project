{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/lucchen/Desktop\n"
          ]
        }
      ],
      "source": [
        "# All imports and setup\n",
        "import os, math, json, sys, warnings\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import h3\n",
        "import pytz\n",
        "import holidays\n",
        "import requests\n",
        "import folium\n",
        "from shapely.geometry import Polygon\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import average_precision_score, brier_score_loss\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Setup\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "os.chdir(\"..\")  # Go to project root\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RAW_CSV = \"data/raw/US_Accidents_March23.csv\"  #change\n",
        "OUT_DIR = \"nb_artifacts\"                       # notebook outputs go here\n",
        "# Los Angeles\n",
        "BBOX = {\n",
        "    \"min_lat\": 33.5,\n",
        "    \"min_lng\": -119.0,\n",
        "    \"max_lat\": 34.9,\n",
        "    \"max_lng\": -117.0\n",
        "}\n",
        "\n",
        "H3_RES = 8\n",
        "\n",
        "# Time window in UTC (panel range)\n",
        "START_UTC = \"2022-01-01T00:00:00Z\"\n",
        "END_UTC   = \"2023-01-01T00:00:00Z\"\n",
        "\n",
        "# Negative sampling fraction (keep this % of negatives)\n",
        "NEG_FRAC = 0.05\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Model params\n",
        "XGB_PARAMS = dict(\n",
        "    n_estimators=400,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.06,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        ")\n",
        "\n",
        "# Evaluation splits (forward chaining)\n",
        "TRAIN_END = \"2022-07-01T00:00:00Z\"\n",
        "VAL_END   = \"2022-10-01T00:00:00Z\"\n",
        "TEST_END  = \"2023-01-01T00:00:00Z\"\n",
        "\n",
        "TOPK = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be572eb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/lucchen/Desktop/accident-risk/notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"..\")  # go up one level from notebooks/ to project root\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec692bb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'data/raw/US_Accidents_March23.csv'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RAW_CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load & filter US‑Accidents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/raw/US_Accidents_March23.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m infra_cols = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAmenity\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mBump\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mCrossing\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGive_Way\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mJunction\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mNo_Exit\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mRailway\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mRoundabout\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mStation\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mStop\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mTraffic_Calming\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mTraffic_Signal\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mTurning_Loop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m ]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# detect what’s present in this CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m header_cols = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.columns.tolist()\n\u001b[32m     15\u001b[39m available_infra = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m infra_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m header_cols]\n\u001b[32m     17\u001b[39m usecols = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m.fromkeys(base_cols + available_infra))  \u001b[38;5;66;03m# de-dup\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/accident-risk/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/accident-risk/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/accident-risk/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/accident-risk/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/accident-risk/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/raw/US_Accidents_March23.csv'"
          ]
        }
      ],
      "source": [
        "#assert os.path.exists(RAW_CSV), f\"Place the Kaggle CSV at: {RAW_CSV}\"\n",
        "\n",
        "# must-have\n",
        "base_cols = [\"ID\",\"Start_Time\",\"Start_Lat\",\"Start_Lng\",\"City\",\"County\",\"State\",\"Timezone\"]\n",
        "\n",
        "# nice-to-have infra columns (some files may miss a few)\n",
        "infra_cols = [\n",
        "    \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
        "  \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
        "]\n",
        "\n",
        "\n",
        "# detect what’s present in this CSV\n",
        "header_cols = pd.read_csv(RAW_CSV, nrows=0).columns.tolist()\n",
        "available_infra = [c for c in infra_cols if c in header_cols]\n",
        "\n",
        "usecols = list(dict.fromkeys(base_cols + available_infra))  # de-dup\n",
        "acc = pd.read_csv(RAW_CSV, usecols=usecols, low_memory=False)\n",
        "len_before = len(acc)\n",
        "\n",
        "# bbox filter\n",
        "acc = acc[(acc.Start_Lat >= BBOX[\"min_lat\"]) & (acc.Start_Lat <= BBOX[\"max_lat\"]) &\n",
        "          (acc.Start_Lng >= BBOX[\"min_lng\"]) & (acc.Start_Lng <= BBOX[\"max_lng\"])].copy()\n",
        "\n",
        "# timestamps\n",
        "acc[\"Start_Time\"] = pd.to_datetime(acc[\"Start_Time\"], utc=True, errors=\"coerce\")\n",
        "acc = acc.dropna(subset=[\"Start_Time\",\"Start_Lat\",\"Start_Lng\"])\n",
        "\n",
        "# time window\n",
        "start_utc = pd.to_datetime(START_UTC, utc=True)\n",
        "end_utc   = pd.to_datetime(END_UTC, utc=True)\n",
        "acc = acc[(acc[\"Start_Time\"] >= start_utc) & (acc[\"Start_Time\"] < end_utc)].copy()\n",
        "\n",
        "print(f\"Loaded {len_before:,} rows; after bbox/time filters: {len(acc):,} rows\")\n",
        "print(\"Infra columns loaded:\", available_infra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d54a4dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "city = \"Los Angeles, California\"\n",
        "url = f\"https://nominatim.openstreetmap.org/search?q={city}&format=json\"\n",
        "\n",
        "headers = {\"User-Agent\": \"accident-risk-demo/1.0 (lucchen@harvard.edu)\"}\n",
        "\n",
        "r = requests.get(url, headers=headers)\n",
        "r.raise_for_status()\n",
        "data = r.json()\n",
        "\n",
        "bbox = data[0][\"boundingbox\"]\n",
        "print(\"Bounding box:\", bbox)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Build H3 grid for bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import h3\n",
        "import pandas as pd\n",
        "import folium\n",
        "\n",
        "# --- 1. Get Los Angeles city boundary polygon from OpenStreetMap ---\n",
        "url = \"https://nominatim.openstreetmap.org/search.php?q=Los+Angeles+California&polygon_geojson=1&format=json\"\n",
        "r = requests.get(url, headers={\"User-Agent\": \"LA-grid-demo\"}).json()\n",
        "geojson_poly = r[0][\"geojson\"]  # Full GeoJSON\n",
        "\n",
        "# --- 2. Create an H3 LatLngPoly shape object (required for H3 4.x) ---\n",
        "coords = geojson_poly[\"coordinates\"][0]  # Outer ring\n",
        "poly = h3.LatLngPoly([(lat, lng) for lng, lat in coords])  # Note: swap lon→lat order\n",
        "\n",
        "# --- 3. Generate hex cells ---\n",
        "cells = list(h3.polygon_to_cells(poly, res=8))  # official new method (pleae dont chaneg this)\n",
        "cells_df = pd.DataFrame({\"h3_id\": cells})\n",
        "print(f\"H3 res=8 → {len(cells_df):,} cells inside Los Angeles\")\n",
        "\n",
        "# --- 4. Visualize ---\n",
        "# --- Visualization ---\n",
        "m = folium.Map(location=[34.05, -118.25], zoom_start=10, tiles=\"cartodb positron\")\n",
        "\n",
        "# \n",
        "for h in cells_df[\"h3_id\"]:\n",
        "    boundary = h3.cell_to_boundary(h)\n",
        "    folium.Polygon(\n",
        "        locations=[(lat, lng) for lat, lng in boundary],\n",
        "        color=\"blue\",\n",
        "        weight=0.5,\n",
        "        fill=True,\n",
        "        fill_opacity=0.3,\n",
        "    ).add_to(m)\n",
        "m\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Build full cell × hour panel and label `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1b54e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Use the LA polygon-derived cells you already built above ---\n",
        "H3_RES = 8  # must match the res you used in polygon_to_cells\n",
        "cells_df = cells_df.drop_duplicates(subset=[\"h3_id\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"[LA] H3 res {H3_RES}: {len(cells_df):,} cells\")\n",
        "\n",
        "# --- Build hours index and panel ---\n",
        "hours = pd.date_range(start=start_utc, end=end_utc, freq=\"H\",\n",
        "                      inclusive=\"left\", tz=\"UTC\")\n",
        "hours_df = pd.DataFrame({\"ts_utc\": hours})\n",
        "\n",
        "cells_df[\"key\"] = 1\n",
        "hours_df[\"key\"] = 1\n",
        "panel = cells_df.merge(hours_df, on=\"key\").drop(columns=[\"key\"])\n",
        "\n",
        "# Ensure tz-aware UTC\n",
        "panel[\"ts_utc\"] = pd.to_datetime(panel[\"ts_utc\"], utc=True)\n",
        "\n",
        "# --- Map accidents to (h3, hour) and count them ---\n",
        "# 1) Compute each accident's h3 cell at the SAME resolution as the grid\n",
        "acc = acc.copy()\n",
        "acc[\"h3_id\"] = acc.apply(\n",
        "    lambda r: h3.latlng_to_cell(r[\"Start_Lat\"], r[\"Start_Lng\"], H3_RES), axis=1\n",
        ")\n",
        "\n",
        "# 2) Keep only accidents that fall inside the LA cell set\n",
        "la_cells = set(cells_df[\"h3_id\"].tolist())\n",
        "acc = acc[acc[\"h3_id\"].isin(la_cells)].copy()\n",
        "\n",
        "# 3) Floor times to hour in UTC (assumes acc[\"Start_Time\"] is already UTC-aware)\n",
        "acc[\"ts_utc\"] = pd.to_datetime(acc[\"Start_Time\"], utc=True).dt.floor(\"H\")\n",
        "\n",
        "# 4) Count accidents per cell-hour\n",
        "acc_counts = (acc.groupby([\"h3_id\", \"ts_utc\"])\n",
        "                .size()\n",
        "                .rename(\"cnt\")\n",
        "                .reset_index())\n",
        "\n",
        "# --- Label merge ---\n",
        "if \"y\" in panel.columns:\n",
        "    panel = panel.drop(columns=[\"y\"])\n",
        "\n",
        "panel = panel.merge(acc_counts, on=[\"h3_id\", \"ts_utc\"], how=\"left\")\n",
        "panel[\"y\"] = (panel[\"cnt\"].fillna(0) > 0).astype(\"int8\")\n",
        "panel = panel.drop(columns=[\"cnt\"])\n",
        "\n",
        "# --- Optional: static road/infrastructure flags (if present in your CSV) ---\n",
        "infra_cols = [\n",
        "    \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
        "    \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
        "]\n",
        "available_infra = [c for c in infra_cols if c in acc.columns]\n",
        "print(\"Infra cols available:\", available_infra)\n",
        "\n",
        "if available_infra:\n",
        "    def to01(s):\n",
        "        return s.map({True:1, False:0, \"True\":1, \"False\":0, 1:1, 0:0}).fillna(0).astype(\"int8\")\n",
        "    for c in available_infra:\n",
        "        acc[c] = to01(acc[c])\n",
        "\n",
        "    static_feats = (\n",
        "        acc.groupby(\"h3_id\")[available_infra]\n",
        "           .mean()                      # fraction of historical crashes with that attribute\n",
        "           .reset_index()\n",
        "    )\n",
        "\n",
        "    panel = panel.merge(static_feats, on=\"h3_id\", how=\"left\")\n",
        "    for c in available_infra:\n",
        "        panel[c] = panel[c].fillna(0).astype(\"float32\")\n",
        "else:\n",
        "    print(\"No infrastructure columns in this CSV; skipping static road features.\")\n",
        "\n",
        "# --- Quick sanity checks ---\n",
        "print(panel[\"y\"].value_counts(dropna=False))\n",
        "\n",
        "# Sample one accident row and verify its panel label\n",
        "if len(acc) > 0:\n",
        "    row = acc.sample(1, random_state=0).iloc[0]\n",
        "    mask = (panel[\"h3_id\"] == row[\"h3_id\"]) & (panel[\"ts_utc\"] == row[\"ts_utc\"])\n",
        "    print(\"Matches for sampled accident:\", mask.sum())\n",
        "    display(panel.loc[mask, [\"h3_id\",\"ts_utc\",\"y\"]].head())\n",
        "else:\n",
        "    print(\"Warning: No accidents landed inside the LA H3 grid for the given time window.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32cbf837",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Time/holiday features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ts = pd.to_datetime(panel[\"ts_utc\"], utc=True)\n",
        "\n",
        "panel[\"hour\"]       = ts.dt.hour.astype(\"int16\")\n",
        "panel[\"dow\"]        = ts.dt.dayofweek.astype(\"int16\")\n",
        "panel[\"month\"]      = ts.dt.month.astype(\"int16\")\n",
        "panel[\"is_weekend\"] = (panel[\"dow\"]>=5).astype(\"int8\")\n",
        "\n",
        "years = sorted(set(ts.dt.year.tolist()))\n",
        "us_holidays = holidays.UnitedStates(years=years)\n",
        "panel[\"is_holiday\"] = ts.dt.date.astype(\"O\").map(lambda d: 1 if d in us_holidays else 0).astype(\"int8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Lag features (computed **strictly** from past values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "panel = panel.sort_values([\"h3_id\",\"ts_utc\"])\n",
        "def compute_lags(g):\n",
        "    y = g[\"y\"].astype(int)\n",
        "    g[\"lag_1h\"] = y.shift(1).fillna(0)\n",
        "    g[\"lag_3h\"] = y.shift(1).rolling(3).sum().fillna(0)\n",
        "    g[\"lag_24h\"]= y.shift(1).rolling(24).sum().fillna(0)\n",
        "    g[\"lag_7d_sum\"]   = y.shift(1).rolling(168, min_periods=1).sum()\n",
        "    g[\"lag_30d_sum\"]  = y.shift(1).rolling(720, min_periods=1).sum()\n",
        "\n",
        "    return g\n",
        "\n",
        "panel = panel.groupby(\"h3_id\", group_keys=False).apply(compute_lags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 get weather data using meteostat integration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc2a3c49",
      "metadata": {},
      "source": [
        "temp_c: Air temperature, measured 2 m above ground level.  Represents the ambient air temperature for that hour.\n",
        "\n",
        "precip_mm: Hourly total precipitation (liquid equivalent of rain, drizzle, snow, etc.). 0 = no precipitation during that hour.\n",
        "\n",
        "wind_kph: Mean wind speed over the hour, measured at 10 m height.\n",
        "\n",
        "pressure_hpa: Mean sea-level air pressure.  Standard atmospheric pressure ≈ 1013 hPa.\n",
        "\n",
        "humidity_pct: Relative humidity – ratio of current air moisture to the maximum possible at that temperature (0–100 %).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# we use meteostat (LA)\n",
        "from meteostat import Stations, Hourly\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Use naive start/end for Meteostat, but keep everything UTC in our panel\n",
        "start_dt = pd.to_datetime(START_UTC, utc=True).tz_convert(\"UTC\").tz_localize(None)\n",
        "end_dt   = pd.to_datetime(END_UTC,   utc=True).tz_convert(\"UTC\").tz_localize(None)\n",
        "\n",
        "# 2) One centroid per cell to find nearest station\n",
        "cell_centroids = (\n",
        "    panel[[\"h3_id\"]].drop_duplicates()\n",
        "    .assign(\n",
        "        lat=lambda df: df[\"h3_id\"].map(lambda h: h3.cell_to_latlng(h)[0]),\n",
        "        lng=lambda df: df[\"h3_id\"].map(lambda h: h3.cell_to_latlng(h)[1]),\n",
        "    )\n",
        ")\n",
        "\n",
        "def nearest_station_id(lat, lng, k=20):\n",
        "    st = Stations().nearby(lat, lng).fetch(k)   # fetch more; LA is dense\n",
        "    return None if st.empty else st.index[0]\n",
        "\n",
        "cell_centroids[\"station_id\"] = [\n",
        "    nearest_station_id(r.lat, r.lng) for r in cell_centroids.itertuples(index=False)\n",
        "]\n",
        "cell_centroids = cell_centroids.dropna(subset=[\"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "# 3) Fetch hourly weather for each station and normalize columns\n",
        "station_ids = sorted(cell_centroids[\"station_id\"].unique().tolist())\n",
        "weather_frames = []\n",
        "for sid in station_ids:\n",
        "    try:\n",
        "        w = Hourly(sid, start_dt, end_dt, model=True).fetch()\n",
        "        if w.empty:\n",
        "            continue\n",
        "\n",
        "        # robust tz: localize if naive, else convert\n",
        "        if getattr(w.index, \"tz\", None) is None:\n",
        "            w.index = w.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            w = w.tz_convert(\"UTC\")\n",
        "\n",
        "        w = w.reset_index().rename(columns={\"time\": \"ts_utc\"})\n",
        "\n",
        "        # Map raw Meteostat names -> our names\n",
        "        rename_map = {\n",
        "            \"temp\": \"temp_c\",       # °C\n",
        "            \"prcp\": \"precip_mm\",    # mm\n",
        "            \"wspd\": \"wind_kph\",     # km/h\n",
        "            \"pres\": \"pressure_hpa\", # hPa\n",
        "            \"rhum\": \"humidity_pct\", # %\n",
        "            \"vsby\": \"vis_km\"        # km\n",
        "        }\n",
        "        for src, dst in rename_map.items():\n",
        "            if src in w.columns:\n",
        "                w[dst] = pd.to_numeric(w[src], errors=\"coerce\")\n",
        "\n",
        "        keep = [\"ts_utc\"] + [v for v in rename_map.values() if v in w.columns]\n",
        "        w = w[keep].copy()\n",
        "        w[\"station_id\"] = sid\n",
        "        weather_frames.append(w)\n",
        "    except Exception as e:\n",
        "        print(f\"[weather] skip {sid}: {e}\")\n",
        "\n",
        "if weather_frames:\n",
        "    weather = pd.concat(weather_frames, ignore_index=True)\n",
        "\n",
        "    # 4) attach station → cell, then merge to panel on (h3_id, ts_utc)\n",
        "    weather_full = (\n",
        "        cell_centroids[[\"h3_id\",\"station_id\"]]\n",
        "        .merge(weather, on=\"station_id\", how=\"left\")\n",
        "        .drop(columns=[\"station_id\"])\n",
        "    )\n",
        "\n",
        "    # ensure UTC & same dtype as panel\n",
        "    weather_full[\"ts_utc\"] = pd.to_datetime(weather_full[\"ts_utc\"], utc=True)\n",
        "\n",
        "    panel = panel.merge(weather_full, on=[\"h3_id\",\"ts_utc\"], how=\"left\")\n",
        "\n",
        "    # 5) Fill reasonable gaps: per-cell ffill/bfill → per-hour citywide median\n",
        "    w_cols = [c for c in [\"temp_c\",\"precip_mm\",\"wind_kph\",\"pressure_hpa\",\n",
        "                          \"humidity_pct\",\"vis_km\"] if c in panel.columns]\n",
        "    panel = panel.sort_values([\"h3_id\",\"ts_utc\"])\n",
        "    for c in w_cols:\n",
        "        panel[c] = (panel.groupby(\"h3_id\")[c]\n",
        "                        .apply(lambda s: s.ffill().bfill())\n",
        "                        .reset_index(level=0, drop=True))\n",
        "        panel[c] = panel[c].fillna(panel.groupby(\"ts_utc\")[c].transform(\"median\"))\n",
        "\n",
        "    # 6) SAFE lags (only from the past) — avoid nowcast leakage\n",
        "    def add_weather_lags(g):\n",
        "        for c in w_cols:\n",
        "            g[f\"{c}_lag1\"] = g[c].shift(1)\n",
        "            if c == \"precip_mm\":\n",
        "                g[f\"{c}_lag3_sum\"]  = g[c].shift(1).rolling(3).sum()\n",
        "                g[f\"{c}_lag24_sum\"] = g[c].shift(1).rolling(24).sum()\n",
        "            else:\n",
        "                g[f\"{c}_lag3_mean\"]  = g[c].shift(1).rolling(3).mean()\n",
        "                g[f\"{c}_lag24_mean\"] = g[c].shift(1).rolling(24).mean()\n",
        "        return g\n",
        "\n",
        "    panel = panel.groupby(\"h3_id\", group_keys=False).apply(add_weather_lags)\n",
        "\n",
        "    # drop contemporaneous weather to keep only lags\n",
        "    #panel = panel.drop(columns=w_cols, errors=\"ignore\")\n",
        "\n",
        "    # quick check\n",
        "    lag_cols = [c for c in panel.columns if c.startswith((\"temp_c_\",\"precip_mm_\",\"wind_kph_\",\n",
        "                                                          \"pressure_hpa_\",\"humidity_pct_\",\"vis_km_\"))]\n",
        "    print(f\"[weather] added lag features: {len(lag_cols)} cols\")\n",
        "else:\n",
        "    print(\"[weather] no station data fetched; skipping weather features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedf3dd5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0435a0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af9f429",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b87e83",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ede4d26",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2ce112",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0f3c7b09",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train XGBoost (weighted) + 9) Calibrate with Isotonic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e099d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd\n",
        "\n",
        "# 1️⃣ Forward time splits\n",
        "TRAIN_END = \"2022-07-01T00:00:00Z\"\n",
        "VAL_END   = \"2022-10-01T00:00:00Z\"\n",
        "TEST_END  = \"2023-01-01T00:00:00Z\"\n",
        "\n",
        "ts = pd.to_datetime(panel[\"ts_utc\"], utc=True)\n",
        "train_end = pd.to_datetime(TRAIN_END, utc=True)\n",
        "val_end   = pd.to_datetime(VAL_END,   utc=True)\n",
        "test_end  = pd.to_datetime(TEST_END,  utc=True)\n",
        "\n",
        "def split_name(t):\n",
        "    if t < train_end: return \"train\"\n",
        "    elif t < val_end: return \"val\"\n",
        "    elif t < test_end: return \"test\"\n",
        "    else: return \"ignore\"\n",
        "\n",
        "panel[\"split\"] = [split_name(t) for t in ts]\n",
        "panel = panel[panel[\"split\"]!=\"ignore\"].copy()\n",
        "\n",
        "# we can do neg sampling later\n",
        "NEG_FRAC = 1\n",
        "np.random.seed(42)\n",
        "\n",
        "train_df = panel[panel[\"split\"]==\"train\"].copy()\n",
        "val_df   = panel[panel[\"split\"]==\"val\"].copy()\n",
        "test_df  = panel[panel[\"split\"]==\"test\"].copy()\n",
        "\n",
        "pos = train_df[train_df[\"y\"]==1]\n",
        "neg = train_df[train_df[\"y\"]==0].sample(frac=NEG_FRAC, random_state=42)\n",
        "train_df = pd.concat([pos, neg], ignore_index=True)\n",
        "\n",
        "# weights: balance the sampling\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"weight\"] = 1.0\n",
        "train_df.loc[train_df[\"y\"]==0, \"weight\"] = 1.0/NEG_FRAC\n",
        "\n",
        "print(f\"Train rows: {len(train_df):,},  Val: {len(val_df):,},  Test: {len(test_df):,}\")\n",
        "print(\"Positives in train:\", (train_df['y']==1).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import average_precision_score, brier_score_loss\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "NUM_COLS = [\"lag_1h\",\"lag_3h\",\"lag_24h\"]\n",
        "infra_cols_present = [c for c in [\n",
        "    \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
        "    \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
        "] if c in panel.columns]\n",
        "NUM_COLS += infra_cols_present\n",
        "\n",
        "CAT_COLS = [\"hour\",\"dow\",\"month\",\"is_weekend\",\"is_holiday\"]\n",
        "\n",
        "def make_xyw(df):\n",
        "    X = df[NUM_COLS + CAT_COLS].copy()\n",
        "    y = df[\"y\"].astype(int).values\n",
        "    w = df[\"weight\"].astype(float).values\n",
        "    return X, y, w\n",
        "\n",
        "X_tr, y_tr, w_tr = make_xyw(train_df)\n",
        "X_va, y_va, w_va = make_xyw(val_df)\n",
        "X_te, y_te, w_te = make_xyw(test_df)\n",
        "\n",
        "pre = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), CAT_COLS)],\n",
        "                        remainder=\"passthrough\")\n",
        "\n",
        "clf = XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    n_estimators=400, max_depth=6, learning_rate=0.06,\n",
        "    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "    eval_metric=\"aucpr\", random_state=42, n_jobs=4\n",
        ")\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"model\", clf)])\n",
        "pipe.fit(X_tr, y_tr, model__sample_weight=w_tr)\n",
        "\n",
        "# Calibrate on validation\n",
        "p_va_raw = pipe.predict_proba(X_va)[:,1]\n",
        "iso = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_raw, y_va, sample_weight=w_va)\n",
        "\n",
        "# Evaluate on test (nowcast of current hour)\n",
        "p_te_raw = pipe.predict_proba(X_te)[:,1]\n",
        "p_te_cal = iso.predict(p_te_raw)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix,\n",
        "    precision_score, recall_score, f1_score\n",
        ")\n",
        "\n",
        "# --- Choose a classification threshold ---\n",
        "# Since accidents are rare, use a small threshold instead of 0.5\n",
        "threshold = 0.01# ujst a quick check\n",
        "y_pred = (p_te_cal >= threshold).astype(int)\n",
        "\n",
        "# --- Compute standard metrics ---\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "prec = precision_score(y_te, y_pred, zero_division=0)\n",
        "rec = recall_score(y_te, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_te, y_pred, zero_division=0)\n",
        "cm = confusion_matrix(y_te, y_pred)\n",
        "\n",
        "print(\"\\n--- Classification Metrics ---\")\n",
        "print(f\"Threshold        : {threshold}\")\n",
        "print(f\"Accuracy         : {acc:.6f}\")\n",
        "print(f\"Precision        : {prec:.6f}\")\n",
        "print(f\"Recall           : {rec:.6f}\")\n",
        "print(f\"F1-score         : {f1:.6f}\")\n",
        "print(\"Confusion Matrix :\")\n",
        "print(pd.DataFrame(cm,\n",
        "                   index=[\"Actual 0\",\"Actual 1\"],\n",
        "                   columns=[\"Pred 0\",\"Pred 1\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35138971",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- ROC–AUC score ---\n",
        "roc_auc = roc_auc_score(y_te, p_te_cal, sample_weight=w_te)\n",
        "print(f\"ROC–AUC: {roc_auc:.6f}\")\n",
        "\n",
        "# --- Optional: visualize ROC curve ---\n",
        "fpr, tpr, thresholds = roc_curve(y_te, p_te_cal, sample_weight=w_te)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random chance\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769d95a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Best-of (LSTM / GRU / TCN) sequence model — pick by val PR-AUC\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------------------\n",
        "# 1) Config\n",
        "# ---------------------\n",
        "W = 168                 # sequence window (hours); try 168 for a week\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 18\n",
        "LR = 1e-3\n",
        "BETA = 2.0              # F-beta for threshold picking on val\n",
        "\n",
        "# Choose features you already have in your panel:\n",
        "NUM_FEATS = [c for c in [\n",
        "    # history (you can keep as-is; no shift policing here)\n",
        "    \"lag_1h\",\"lag_3h\",\"lag_24h\",\"lag_7d_sum\",\"lag_30d_sum\",\"lag_ewm_24h\",\"lag_ewm_7d\",\n",
        "    # neighbor spillover (if created; ok to omit if not in panel)\n",
        "    \"k1_lag_1h\",\"k1_lag_3h\",\"k1_lag_24h\",\"k1_lag_7d_sum\",\n",
        "    \"k2_lag_1h\",\"k2_lag_3h\",\"k2_lag_24h\",\"k2_lag_7d_sum\",\n",
        "    # weather (current+lags ok, we’re ignoring leakage)\n",
        "    \"temp_c\",\"temp_c_lag1\",\"temp_c_lag3_mean\",\"temp_c_lag24_mean\",\"temp_c_diff1\",\"temp_c_anom24\",\n",
        "    \"wind_kph\",\"wind_kph_lag1\",\"wind_kph_lag3_mean\",\"wind_kph_lag24_mean\",\"wind_kph_diff1\",\"wind_kph_anom24\",\n",
        "    \"pressure_hpa\",\"pressure_hpa_lag1\",\"pressure_hpa_lag3_mean\",\"pressure_hpa_lag24_mean\",\"pressure_hpa_diff1\",\"pressure_hpa_anom24\",\n",
        "    \"humidity_pct\",\"humidity_pct_lag1\",\"humidity_pct_lag3_mean\",\"humidity_pct_lag24_mean\",\"humidity_pct_diff1\",\"humidity_pct_anom24\",\n",
        "    \"precip_mm\",\"precip_mm_lag3_sum\",\"precip_mm_lag24_sum\",\"rain_flag_3h\",\"rain_flag_24h\",\n",
        "] if c in panel.columns]\n",
        "\n",
        "CAT_FEATS = [\"hour\",\"dow\",\"month\",\"is_weekend\",\"is_holiday\"]\n",
        "CAT_FEATS = [c for c in CAT_FEATS if c in panel.columns]\n",
        "\n",
        "STATIC_FEATS = [c for c in [\n",
        "    \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
        "    \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
        "] if c in panel.columns]\n",
        "\n",
        "# ---------------------\n",
        "# 2) Prepare DataFrame\n",
        "# ---------------------\n",
        "need_cols = [\"h3_id\",\"ts_utc\",\"y\"] + NUM_FEATS + CAT_FEATS + STATIC_FEATS + [\"split\"]\n",
        "df = panel[need_cols].copy()\n",
        "df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
        "df = df.sort_values([\"h3_id\",\"ts_utc\"]).reset_index(drop=True)\n",
        "\n",
        "# map cells\n",
        "h3_list = df[\"h3_id\"].drop_duplicates().tolist()\n",
        "h3_to_idx = {h:i for i,h in enumerate(h3_list)}\n",
        "df[\"h3_idx\"] = df[\"h3_id\"].map(h3_to_idx).astype(\"int32\")\n",
        "\n",
        "# normalize numeric (fit on train only)\n",
        "train_mask = df[\"split\"]==\"train\"\n",
        "num_stats = {}\n",
        "for c in NUM_FEATS:\n",
        "    m = pd.to_numeric(df.loc[train_mask, c], errors=\"coerce\").astype(\"float32\")\n",
        "    mu, sd = float(m.mean()), float(m.std() if m.std()>1e-6 else 1.0)\n",
        "    num_stats[c] = (mu, sd)\n",
        "    df[c] = ((pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\") - mu)/sd).fillna(0.0)\n",
        "\n",
        "for c in CAT_FEATS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "for c in STATIC_FEATS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(\"float32\")\n",
        "\n",
        "groups = {gid: g for gid, g in df.groupby(\"h3_idx\", sort=True)}\n",
        "\n",
        "# ---------------------\n",
        "# 3) Dataset\n",
        "# ---------------------\n",
        "class CellSeqDataset(Dataset):\n",
        "    def __init__(self, groups, split, W, num_cols, cat_cols, static_cols):\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.static_cols = static_cols\n",
        "        self.W = W\n",
        "        self.samples = []\n",
        "        for h3_idx, g in groups.items():\n",
        "            g = g[g[\"split\"]==split]\n",
        "            if len(g) < W: \n",
        "                continue\n",
        "            arr_num = g[num_cols].to_numpy(np.float32)\n",
        "            arr_cat = g[cat_cols].to_numpy(np.int64) if cat_cols else None\n",
        "            arr_static = g[static_cols].to_numpy(np.float32) if static_cols else None\n",
        "            y = g[\"y\"].astype(\"int64\").to_numpy()\n",
        "            for t in range(W-1, len(g)):\n",
        "                Xn = arr_num[t-W+1:t+1]\n",
        "                Xc = arr_cat[t-W+1:t+1] if arr_cat is not None else None\n",
        "                Xs = arr_static[t] if arr_static is not None else None\n",
        "                self.samples.append((h3_idx, Xn, Xc, Xs, y[t]))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i):\n",
        "        h3_idx, Xn, Xc, Xs, y = self.samples[i]\n",
        "        return (\n",
        "            torch.tensor(h3_idx, dtype=torch.long),\n",
        "            torch.tensor(Xn, dtype=torch.float32),\n",
        "            (torch.tensor(Xc, dtype=torch.long) if Xc is not None else torch.empty(0, dtype=torch.long)),\n",
        "            (torch.tensor(Xs, dtype=torch.float32) if Xs is not None else torch.empty(0)),\n",
        "            torch.tensor(y, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "def collate(batch):\n",
        "    h_idx, Xn, Xc, Xs, y = zip(*batch)\n",
        "    Xn = torch.stack(Xn)  # (B,W,Fnum)\n",
        "    if Xc[0].numel() == 0:\n",
        "        Xc = None\n",
        "    else:\n",
        "        Xc = torch.stack(Xc)  # (B,W,C)\n",
        "    if Xs[0].numel() == 0:\n",
        "        Xs = None\n",
        "    else:\n",
        "        Xs = torch.stack(Xs)  # (B,Fstatic)\n",
        "    y = torch.stack(y)\n",
        "    return (torch.tensor(h_idx, dtype=torch.long), Xn, Xc, Xs, y)\n",
        "\n",
        "train_ds = CellSeqDataset(groups, \"train\", W, NUM_FEATS, CAT_FEATS, STATIC_FEATS)\n",
        "val_ds   = CellSeqDataset(groups, \"val\",   W, NUM_FEATS, CAT_FEATS, STATIC_FEATS)\n",
        "test_ds  = CellSeqDataset(groups, \"test\",  W, NUM_FEATS, CAT_FEATS, STATIC_FEATS)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# ---------------------\n",
        "# 4) Models: LSTM / GRU / TCN\n",
        "# ---------------------\n",
        "class CatEmb(nn.Module):\n",
        "    def __init__(self, cardinals, emb_dim=8):\n",
        "        super().__init__()\n",
        "        self.embs = nn.ModuleList([nn.Embedding(c, min(emb_dim, max(2,c))) for c in cardinals])\n",
        "        self.out_dim = sum(e.embedding_dim for e in self.embs)\n",
        "    def forward(self, Xc):  # (B,W,C)\n",
        "        if Xc is None: return None\n",
        "        embs = [emb(Xc[:,:,i]) for i, emb in enumerate(self.embs)]  # list of (B,W,Di)\n",
        "        return torch.cat(embs, dim=-1)  # (B,W,sumD)\n",
        "\n",
        "class RNNSeq(nn.Module):\n",
        "    def __init__(self, kind, num_in, cat_card=None, static_in=0, hid=128, layers=1):\n",
        "        super().__init__()\n",
        "        self.cat = CatEmb(cat_card) if cat_card else None\n",
        "        in_dim = num_in + (self.cat.out_dim if self.cat else 0)\n",
        "        rnn = {\"lstm\": nn.LSTM, \"gru\": nn.GRU}[kind]\n",
        "        self.rnn = rnn(in_dim, hid, num_layers=layers, batch_first=True)\n",
        "        head_in = hid + static_in\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(head_in, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.25),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, Xn, Xc=None, Xs=None):\n",
        "        # Xn: (B,W,F)\n",
        "        if self.cat:\n",
        "            Ec = self.cat(Xc)                       # (B,W,De)\n",
        "            x = torch.cat([Xn, Ec], dim=-1)\n",
        "        else:\n",
        "            x = Xn\n",
        "        seq, _ = self.rnn(x)                        # (B,W,H)\n",
        "        h = seq[:,-1,:]                             # (B,H)\n",
        "        if Xs is not None: h = torch.cat([h, Xs], dim=1)\n",
        "        logit = self.head(h).squeeze(1)\n",
        "        return torch.sigmoid(logit)\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size): super().__init__(); self.chomp_size=chomp_size\n",
        "    def forward(self, x): return x[:,:,:-self.chomp_size]\n",
        "\n",
        "class TCNBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, d=1, p=0.2):\n",
        "        super().__init__()\n",
        "        pad = (k-1)*d\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, k, padding=pad, dilation=d),\n",
        "            Chomp1d(pad), nn.ReLU(), nn.BatchNorm1d(out_ch), nn.Dropout(p),\n",
        "            nn.Conv1d(out_ch, out_ch, k, padding=pad, dilation=d),\n",
        "            Chomp1d(pad), nn.ReLU(), nn.BatchNorm1d(out_ch), nn.Dropout(p),\n",
        "        )\n",
        "        self.down = nn.Conv1d(in_ch, out_ch, 1) if in_ch!=out_ch else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out + self.down(x)\n",
        "\n",
        "class TCNSeq(nn.Module):\n",
        "    def __init__(self, num_in, cat_card=None, static_in=0, hid=128, levels=4, k=3):\n",
        "        super().__init__()\n",
        "        self.cat = CatEmb(cat_card) if cat_card else None\n",
        "        in_dim = num_in + (self.cat.out_dim if self.cat else 0)\n",
        "        chs = [in_dim] + [hid]*levels\n",
        "        blocks = []\n",
        "        for i in range(levels):\n",
        "            blocks.append(TCNBlock(chs[i], chs[i+1], k=k, d=2**i, p=0.2))\n",
        "        self.tcn = nn.Sequential(*blocks)\n",
        "        head_in = hid + static_in\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(head_in, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.25),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, Xn, Xc=None, Xs=None):\n",
        "        # Xn: (B,W,F) -> transpose to (B,F,W)\n",
        "        if self.cat:\n",
        "            Ec = self.cat(Xc)                # (B,W,De)\n",
        "            x = torch.cat([Xn, Ec], dim=-1)  # (B,W,F+De)\n",
        "        else:\n",
        "            x = Xn\n",
        "        x = x.transpose(1,2)                 # (B, Fch, W)\n",
        "        y = self.tcn(x)                      # (B, hid, W)\n",
        "        h = y[:,:,-1]                        # (B, hid)\n",
        "        if Xs is not None: h = torch.cat([h, Xs], dim=1)\n",
        "        logit = self.head(h).squeeze(1)\n",
        "        return torch.sigmoid(logit)\n",
        "\n",
        "# ---------------------\n",
        "# 5) Train & Select\n",
        "# ---------------------\n",
        "def run_train(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    bce = nn.BCELoss()\n",
        "    best = {\"pr\": -1, \"state\": None}\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        for _, Xn, Xc, Xs, y in train_loader:\n",
        "            Xn, y = Xn.to(DEVICE), y.to(DEVICE)\n",
        "            if Xc is not None: Xc = Xc.to(DEVICE)\n",
        "            if Xs is not None: Xs = Xs.to(DEVICE)\n",
        "            p = model(Xn, Xc, Xs)\n",
        "            loss = bce(p, y)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "        # val PR-AUC\n",
        "        model.eval(); pv, yv = [], []\n",
        "        with torch.no_grad():\n",
        "            for _, Xn, Xc, Xs, y in val_loader:\n",
        "                Xn = Xn.to(DEVICE)\n",
        "                if Xc is not None: Xc = Xc.to(DEVICE)\n",
        "                if Xs is not None: Xs = Xs.to(DEVICE)\n",
        "                p = model(Xn, Xc, Xs).detach().cpu().numpy()\n",
        "                pv.append(p); yv.append(y.numpy())\n",
        "        pv = np.concatenate(pv) if pv else np.array([])\n",
        "        yv = np.concatenate(yv) if yv else np.array([])\n",
        "        pr = average_precision_score(yv, pv) if (yv.size and len(np.unique(yv))>1) else 0.0\n",
        "        if pr > best[\"pr\"]:\n",
        "            best = {\"pr\": pr, \"state\": {k:v.cpu().clone() for k,v in model.state_dict().items()}}\n",
        "        print(f\"[{model.__class__.__name__}] epoch {ep:02d}  val PR-AUC={pr:.6f}\")\n",
        "    model.load_state_dict({k:v.to(DEVICE) for k,v in best[\"state\"].items()})\n",
        "    return model, best[\"pr\"]\n",
        "\n",
        "cat_card = [24,7,12,2,2] if CAT_FEATS else None\n",
        "c_static = len(STATIC_FEATS)\n",
        "\n",
        "candidates = {\n",
        "    \"LSTM\": RNNSeq(kind=\"lstm\", num_in=len(NUM_FEATS), cat_card=cat_card, static_in=c_static, hid=160, layers=1),\n",
        "    \"GRU\" : RNNSeq(kind=\"gru\",  num_in=len(NUM_FEATS), cat_card=cat_card, static_in=c_static, hid=160, layers=1),\n",
        "    \"TCN\" : TCNSeq(num_in=len(NUM_FEATS), cat_card=cat_card, static_in=c_static, hid=192, levels=4, k=3),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "best_name, best_model, best_val = None, None, -1\n",
        "for name, mdl in candidates.items():\n",
        "    print(f\"\\n==== Train {name} ====\")\n",
        "    m, val_pr = run_train(mdl, train_loader, val_loader, epochs=EPOCHS, lr=LR)\n",
        "    results[name] = val_pr\n",
        "    if val_pr > best_val:\n",
        "        best_val, best_name, best_model = val_pr, name, m\n",
        "\n",
        "print(\"\\nValidation PR-AUC per model:\", {k: round(v,6) for k,v in results.items()})\n",
        "print(\"→ Selected:\", best_name, \"with PR-AUC\", round(best_val,6))\n",
        "\n",
        "# ---------------------\n",
        "# 6) Test evaluation\n",
        "# ---------------------\n",
        "best_model.eval(); pt, yt = [], []\n",
        "with torch.no_grad():\n",
        "    for _, Xn, Xc, Xs, y in test_loader:\n",
        "        Xn = Xn.to(DEVICE)\n",
        "        if Xc is not None: Xc = Xc.to(DEVICE)\n",
        "        if Xs is not None: Xs = Xs.to(DEVICE)\n",
        "        p = best_model(Xn, Xc, Xs).detach().cpu().numpy()\n",
        "        pt.append(p); yt.append(y.numpy())\n",
        "pt = np.concatenate(pt) if pt else np.array([])\n",
        "yt = np.concatenate(yt) if yt else np.array([])\n",
        "\n",
        "test_pr = average_precision_score(yt, pt) if (yt.size and len(np.unique(yt))>1) else 0.0\n",
        "print(\"Test PR-AUC (prob ranking):\", round(float(test_pr), 6))\n",
        "\n",
        "# pick threshold on VAL by F-beta\n",
        "best_model.eval(); pv, yv = [], []\n",
        "with torch.no_grad():\n",
        "    for _, Xn, Xc, Xs, y in val_loader:\n",
        "        Xn = Xn.to(DEVICE)\n",
        "        if Xc is not None: Xc = Xc.to(DEVICE)\n",
        "        if Xs is not None: Xs = Xs.to(DEVICE)\n",
        "        pv.append(best_model(Xn, Xc, Xs).detach().cpu().numpy()); yv.append(y.numpy())\n",
        "pv = np.concatenate(pv) if pv else np.array([])\n",
        "yv = np.concatenate(yv) if yv else np.array([])\n",
        "\n",
        "prec, rec, thr = precision_recall_curve(yv, pv)\n",
        "fb = (1+BETA**2)*prec*rec/(BETA**2*prec + rec + 1e-12)\n",
        "best_t = float(thr[np.nanargmax(fb[:-1])]) if thr.size else 0.5\n",
        "print(\"Chosen threshold (val, Fβ):\", best_t)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, brier_score_loss\n",
        "y_pred = (pt >= best_t).astype(int)\n",
        "print(\"Test precision:\", round(float(precision_score(yt, y_pred, zero_division=0)),6))\n",
        "print(\"Test recall   :\", round(float(recall_score(yt, y_pred, zero_division=0)),6))\n",
        "print(\"Test F1       :\", round(float(f1_score(yt, y_pred, zero_division=0)),6))\n",
        "print(\"Brier score   :\", round(float(brier_score_loss(yt, np.clip(pt,1e-6,1-1e-6))),6))\n",
        "print(\"Confusion:\\n\", pd.DataFrame(confusion_matrix(yt, y_pred),\n",
        "                                    index=[\"Actual 0\",\"Actual 1\"],\n",
        "                                    columns=[\"Pred 0\",\"Pred 1\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04193971",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b8e5ff",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc50e0ba",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
